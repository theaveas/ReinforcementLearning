{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term `dynamic programming` (DP) refers to a *collection of algorithms that can be used to compute optimal polices given a perfect model of the environmen as MDPs*. Classical DP algorithms are of *limited* utility in RL both because of **their assumption of a perfect model** and **their great computational expense**.\\\n",
    "The key isea of DP, and of RL generally, is useof *value functions* to organize and structure the search for good policies. \\\n",
    "Remind of `optimal value function` and `optimal action-value functions`:\n",
    "$$ v_{*}(s) = \\max\\limits_{a}\\sum\\limits_{s',r}p(s',r|s,a)[r + \\gamma v_{*}(s)] \\; \\text{for all} s \\in S, a \\in A$$\n",
    "$$ q_{*}(s, a) = \\sum\\limits_{s',s}p(s',r|s,a)[r + \\gamma\\max\\limits_{a'}q_{*}(s',a')] \\; \\text{for all} s \\in S, a \\in A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Prediction)\n",
    "First we consider how to compute the state-value function $v_{*}$ for an *arbitrary* policy $\\pi$. This is called `Policy Evaluation`, we also refer to *prediction problem*.\\\n",
    "**Recall**\n",
    "$$v_{\\pi}(s) \\doteq \\sum\\limits_{a}\\pi(a|s)\\sum\\limits_{s',r|s,a}p(s',r|s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\\\n",
    "Consider a sequence of approximate value functions $v_{0},v_{1},v_{2},...,$ each mapping $S'$ to the real numbers. The initial approxiamtion, $v_{0}$ is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the `Bellman equation`. The sequence $v_{k}$ can be shown in general to vonverge to $v_{\\pi}$ as $k \\leftarrow \\infty$ under the same condistions that gurantee the existence of $v_{pi}$. This algorithm is called **`iteration policy evaluation`**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Policy Evaluation Pseudocode\n",
    "Input $\\pi$ the policy to be evaluated\\\n",
    "Algorithm parameter: a small thershold $\\delta > 0$ determining accuracy of estimation\\\n",
    "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\\\n",
    "\\\n",
    "**Loop**:\\\n",
    "$\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "$\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "$\\;\\;\\;\\;$ **until** $\\triangle \\leftarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency of Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f314e49fb3c2842732be35764892ca5d931d280188619711627898b2880bf289"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
