{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term `dynamic programming` (DP) refers to a *collection of algorithms that can be used to compute optimal polices given a perfect model of the environmen as MDPs*. Classical DP algorithms are of *limited* utility in RL both because of **their assumption of a perfect model** and **their great computational expense**.\\\n",
    "The key isea of DP, and of RL generally, is useof *value functions* to organize and structure the search for good policies. \\\n",
    "Remind of `optimal value function` and `optimal action-value functions`:\n",
    "$$ v_{*}(s) = \\max\\limits_{a}\\sum\\limits_{s',r}p(s',r|s,a)[r + \\gamma v_{*}(s)] \\; \\text{for all} s \\in S, a \\in A$$\n",
    "$$ q_{*}(s, a) = \\sum\\limits_{s',s}p(s',r|s,a)[r + \\gamma\\max\\limits_{a'}q_{*}(s',a')] \\; \\text{for all} s \\in S, a \\in A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Prediction)\n",
    "First we consider how to compute the state-value function $v_{*}$ for an *arbitrary* policy $\\pi$. This is called `Policy Evaluation`, we also refer to *prediction problem*.\\\n",
    "**Recall**\n",
    "$$v_{\\pi}(s) \\doteq \\sum\\limits_{a}\\pi(a|s)\\sum\\limits_{s',r|s,a}p(s',r|s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\\\n",
    "Consider a sequence of approximate value functions $v_{0},v_{1},v_{2},...,$ each mapping $S'$ to the real numbers. The initial approxiamtion, $v_{0}$ is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the `Bellman equation`. The sequence $v_{k}$ can be shown in general to vonverge to $v_{\\pi}$ as $k \\leftarrow \\infty$ under the same condistions that gurantee the existence of $v_{pi}$. This algorithm is called **`iteration policy evaluation`**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Policy Evaluation Pseudocode\n",
    "Input $\\pi$ the policy to be evaluated\\\n",
    "Algorithm parameter: a small thershold $\\theta > 0$ determining accuracy of estimation\\\n",
    "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\\\n",
    "\\\n",
    "**Loop**:\\\n",
    "$\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "$\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "$\\;\\;\\;\\;$ **until** $\\triangle < \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "The reason for computing the value function for a policy is to help find better policies. For some state $s$ we would like to know whether or not we should change the policy to deterministically choose an action $a \\neq \\pi(s)$. We know how good it is to follow the current policy frome $s$ -- but would it be better or worse to change to the new policy?\\\n",
    "The key criterion is whether this is greater than or less than $v_{\\pi}(s)$. If it is `greater` that is, if it is better to select $a$ once in $s$ and therefore follow $\\pi$ than it would be to follow $\\pi$ all the time. Then one would expect it to be better still to select $a$ every time $s$ is encountered, and that the new policy would in fact be a better one overall.\\\n",
    "That this is treue is a special case of a general result called the **`policy improvement`**. Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$:\n",
    "$$q_{\\pi}(s', \\pi') \\geq v_{\\pi}(s)$$\n",
    "Then the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all state $s \\in S$:\n",
    "$$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration Evaluation Pseudocode\n",
    "1. Initialization\\\n",
    "   $V(s) \\in \\real$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$\\\n",
    "\n",
    "2. Policy Evaluation\\\n",
    "   **Loop**:\\\n",
    "   $\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "   $\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "   $\\;\\;\\;\\;$ **until** $\\triangle < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\\\n",
    "   *policy-stable* $ \\; \\leftarrow$ *true*\\\n",
    "   **For each** $s \\in S$:\\\n",
    "   $\\;\\;\\;\\;$ *old-action* $ \\; \\leftarrow \\pi(s)$\\\n",
    "   $\\;\\;\\;\\;$ $\\pi(s) \\leftarrow \\argmax_{a}\\sum_{s',r}p(s',r|p,s)[r + \\gamma V(s')]$\\\n",
    "   $\\;\\;\\;\\;$**if** *old-action* $ \\neq \\pi(s)$:\\\n",
    "   $\\;\\;\\;\\;$ $\\;\\;\\;\\;$ *policy-stable* $\\leftarrow $ false\\\n",
    "   **if** *policy-stable*:\\\n",
    "   $\\;\\;\\;\\;$  **return** *optimal value function* and *optimal policy* \\\n",
    "   **else**:\\\n",
    "   $\\;\\;\\;\\;$ Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration Pseudocode\n",
    "Algorithm parameter: a small thershold $\\theta > 0$ determining accuracy of estimation\\\n",
    "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\\\n",
    "\\\n",
    "**Loop**:\\\n",
    "$\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "$\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\max_{a}\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "$\\;\\;\\;\\;$ **until** $\\triangle < \\theta$\\\n",
    "**Output**: a deterministic optimal policy $\\pi$,\\\n",
    "$\\;\\;\\;\\;$ $\\pi(s) = \\argmax_{a}\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency of Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f314e49fb3c2842732be35764892ca5d931d280188619711627898b2880bf289"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
