{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes (MDPs)\n",
    "`MDPs` are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through future rewards. In `MDPs` we *estimate* the value $q_{*}(s, a)$ of each action $a$ in each state $s$, or we *esitmate* value $v_{*}(s)$ of each state given the optimal action selections.\n",
    "\n",
    "### The Agent-Environment Interface\n",
    "`MDPS` are meant to be a straightforward framing of the problem of learning from interaction to achieve the goal. **`Agent`** who is the learner and decision maker. The things it *interacts with* comprising everything outside the agent, is called **`Environment`**.\\\n",
    "This interaction continually, the agent selecting actions and the environment responding to these actions and presenting new situations(states) to the agent. The environment also given rise to rewards, that the agent seeks to cumulative over time through its choice of actions.\n",
    "\\\n",
    "\\\n",
    "The *agent* and *environment* interact at each of a `sequence of discrete time steps,` $t = 0.1.2...N$ At each timestep $t$, *the agent* receives some representation of the environment's state $S_{t} \\in S$, and on that basis selects an action, $A_{t} \\in A(s)$. One time step later, in part as a consequence of its action $A{t}, *the agent* receives a numerical reward $R_{t+1} \\in R$, and finds itself in a new state, $S_{t+1}$. \n",
    "\\\n",
    "The MDP and agent together thereby give rise to a sequence or `trajectory` that begins like this:\n",
    "$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2},  S_{2}, A_{2}, R_{3}, ...$$ \n",
    "\\\n",
    "In *finite MDPs*, the sets of states, actions, and rewards ($S, A, and R$) all have a finite number of elements. In this case, the *random variables* $R_{t} and S_{t}$ have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these *random variables*, $s' \\in S and r \\in R$, there is a probability of those values occuring at time $t$, given particular values of the preceding state and action:\n",
    "$$p(s', r | s, a) \\doteq Pr({S_{t} = s', R_{t} = r | S_{t-1} = s, A_{t-1} = a})$$\n",
    "To simplified this formular, remember that $p$ specifies a probability distribution for each choice of given state $s$ and action $a$ that is, that:\n",
    "$$ \\sum \\limits_{s' \\in S} \\sum\\limits_{r \\in R} p(s', r | s, a) \\doteq 1, \\; for \\; all \\; s \\in S a \\in A$$\n",
    "\n",
    "### Goals and Rewards\n",
    "In RL, the purpose or goal of the agent is formalized in terms of special signal, called the **`reward`**, passing from the environment to the agent. Informally, the agent's *goal* is `to maximize the total amount of reward` it receives. This means maximizing not *immediate reward*, but *cumulative reward* in the long run.\n",
    "> That all of what we mean by foals and purposes can be well thought of as the maximization of the *expected value* of the cumulative sum of a received scalar signal (called reward). >\n",
    "\n",
    "`The use of a reward signal` to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.\\\n",
    "The reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want to do. Than main goal is to reach the end goal not subgoals. If achieving these of subgoals were rewarded, then the agentmight find a way to achieved them without achieving the end goal.\n",
    "\n",
    "### Returns and Episodes\n",
    "- In general, we seek to maximize the *expected return*, where the return, denoted $G_{t}$ is deined as some specific function of the reward sequence (the sum of the rewards):\n",
    "$$ G_{t} = R_{1} + R_{2} + R_{3} + ... + R_{T}$$\n",
    "- When the agent-environment interaction breaks naturally into subsequences, which we call **`episodes`**. Each episode ends in a special state called **`the terminal state`**, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. \n",
    "\n",
    "Thus the episodes can all be sonsidered to end in the same terminal state, with different rewards for tht different outcomes. Tasks with episodes of this kind are called **`episodic tasks`**. Differently if the agent-environmenrt interaction does not break naturally into identifiable episodes, but goes on continually without limit we called it **`continuing tasks`**. \\\n",
    "The return formulation os problematic for continuing tasks becauase the final time step would be $T = \\infty$, and return, which is wahe we are trying to maximize, could itself easily be infinite. To solve this we introduce *discount factor*:\n",
    "$$ G_{t} \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R{t+3} + ...  = \\sum\\limits_{k-0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "where $\\gamma$ is a parameter, 0 $\\leq \\gamma \\leq$ 1, called **`discount factor`**. *This disocunt rate determines the present value of future rewardsL a reward received $k$ time steps in the future is worth only $\\gamma^k-1$ times what it would be worth if it were reveived immediately.* As $\\gamma$ approcaches 1, the return objective takes future rewards into account more strongly; the agent becomes more *farsighted*.\\\n",
    "\\\n",
    "Return at succesive time steps are related to each other in a way that is important for the teory and algorithms of reinforcement learning:\n",
    "$$G_{t} \\doteq R_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "### Unified Notation for Episodic and Continuing Tasks\n",
    "We need to consider a series of episodes, each of which consists of *infinite sequence of timesteps*. We have to refer the state representation at time $t$ of episode $i$ (and similarly for $A_{t,i}, R_{t,i}, \\pi_{t,i}, T_{i}$, etc.). We need one other convention to obtain a single notation that covers both *episodic* and *continuing* task. We have defined the return as sum over a finite number of terms and as a sum over an infinite nber of terms in the other. These two can be *unified* by considering episode termination to be the entering of a special *obsorbing state* that transitions only to itself and that generates only rewards of zero. Alternatively we can write:\n",
    "$$G_{t} \\doteq \\sum\\limits{k=t+}^T \\gamma^k-t-1 R_{k}$$\n",
    "\n",
    "\n",
    "### Policies and Value Funtions\n",
    "Reinforcement Learning algorithms involve *estimating value funtion*- functions of states (or of state-action pairs) that esitmate *how good* it is for the agent to be in a given state (or how good it is to perform a given action in a given state). `How good` here is defined in ters of future rewards that can be expected. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, `Value Functions` is are defined with repect to particular ways of active, called `policies`.\\\n",
    "- A **`Policy`** is a mapping from states to probabilities of selecting each posible action. If the agen is following policy $\\pi$ at time $t$, then $\\pi{t}(a | s)$ is the probability that $A_{t} = a, S_{t} = s$.\n",
    "- The **`Value function`** of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$ is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_{\\pi}(s)$ by:\n",
    "$$v_{\\pi}(s) \\doteq E_{\\pi}[G_{t}|S_{t} = s] = E_{\\pi}[\\sum\\limits_{k=0}^\\infty \\gamma^k R_{t+k+1}| S_{t}=s], \\; \\text{for all} \\; s \\in S$$\n",
    "\n",
    "- Similarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_{\\pi}(s, a)$, as the epected return starting from $s$, taking the action $a$, and thereafter following the policy $\\pi$:\n",
    "$$q_{\\pi}(s|a) \\doteq E_{pi}[G_{t} | S_{t}=s, A_{t}=a] = E_{\\pi}[\\sum\\limits_{k=0}^]infty \\gamma^k R_{t+k+1}| S_{t}=s, A_{t}=t]$$\n",
    "\n",
    "Fundamental property of value functions used throughout reinforcement learning and dynamic programing is that ther stisfy recrsive relationships. For any policy $\\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states (**`Bellman equation of`**):\n",
    "$$v_{\\pi} \\doteq \\sum\\limits_{a} \\pi(a|s) \\sum\\limits_{s',r}p(s',r|s, a)[\\gamma + \\gamma v_{\\pi}(s')], \\; \\text{for all} \\; s \\in S$$\n",
    "\n",
    "### Optimal Policies and Optimal Value Fnctions\n",
    "We can define an optimal policy by: defined a better policy $\\pi$ to be greater than or equal $\\pi'$ if its expected return is greater than or equal to that of $\\pi'$ for all states. There is always at east one policy that is better than or equal to all other policies. Although there may be more than one optimal policies, we donoted by $\\pi_{*}$.\\\n",
    "They share the smae state-value function, call **`optimal state-value function`**, denoted $v_{*}(s)$ defined as:\n",
    "$$v_{*}(s) \\doteq \\max\\limits_{\\pi}v_{\\pi}(s)$$\n",
    "Optimal policies also share the same **`optimal action-value function`**, denoted $q_{*}(s, a)$ defined as:\n",
    "$$q_{*}(s, a) \\doteq \\max\\limits_{\\pi}q_{\\pi}(s, a)$$\n",
    "for all $s \\in S$ and $a \\in A(s)$, For the state-action pair $(s, a)$, ths function gives us the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, er can write $q_{*}$ in terms of $v_{*}$ as:\n",
    "$$q_{*}(s, a) \\doteq E[R_{t+1} + \\gamma v_{*}(S_{t+1}) | S_{t}=s, A_{t} = a] $$\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f314e49fb3c2842732be35764892ca5d931d280188619711627898b2880bf289"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
