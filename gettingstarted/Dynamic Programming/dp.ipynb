{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming (DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term `dynamic programming` (DP) refers to a *collection of algorithms that can be used to compute optimal polices given a perfect model of the environmen as MDPs*. Classical DP algorithms are of *limited* utility in RL both because of **their assumption of a perfect model** and **their great computational expense**.\\\n",
    "The key isea of DP, and of RL generally, is useof *value functions* to organize and structure the search for good policies. \\\n",
    "Remind of `optimal value function` and `optimal action-value functions`:\n",
    "$$ v_{*}(s) = \\max\\limits_{a}\\sum\\limits_{s',r}p(s',r|s,a)[r + \\gamma v_{*}(s)] \\; \\text{for all} s \\in S, a \\in A$$\n",
    "$$ q_{*}(s, a) = \\sum\\limits_{s',s}p(s',r|s,a)[r + \\gamma\\max\\limits_{a'}q_{*}(s',a')] \\; \\text{for all} s \\in S, a \\in A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Prediction)\n",
    "First we consider how to compute the state-value function $v_{*}$ for an *arbitrary* policy $\\pi$. This is called `Policy Evaluation`, we also refer to *prediction problem*.\\\n",
    "**Recall**\n",
    "$$v_{\\pi}(s) \\doteq \\sum\\limits_{a}\\pi(a|s)\\sum\\limits_{s',r|s,a}p(s',r|s, a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\\\n",
    "Consider a sequence of approximate value functions $v_{0},v_{1},v_{2},...,$ each mapping $S'$ to the real numbers. The initial approxiamtion, $v_{0}$ is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the `Bellman equation`. The sequence $v_{k}$ can be shown in general to vonverge to $v_{\\pi}$ as $k \\leftarrow \\infty$ under the same condistions that gurantee the existence of $v_{pi}$. This algorithm is called **`iteration policy evaluation`**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Policy Evaluation Pseudocode\n",
    "Input $\\pi$ the policy to be evaluated\\\n",
    "Algorithm parameter: a small thershold $\\theta > 0$ determining accuracy of estimation\\\n",
    "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\\\n",
    "\\\n",
    "**Loop**:\\\n",
    "$\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "$\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "$\\;\\;\\;\\;$ **until** $\\triangle < \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "The reason for computing the value function for a policy is to help find better policies. For some state $s$ we would like to know whether or not we should change the policy to deterministically choose an action $a \\neq \\pi(s)$. We know how good it is to follow the current policy frome $s$ -- but would it be better or worse to change to the new policy?\\\n",
    "The key criterion is whether this is greater than or less than $v_{\\pi}(s)$. If it is `greater` that is, if it is better to select $a$ once in $s$ and therefore follow $\\pi$ than it would be to follow $\\pi$ all the time. Then one would expect it to be better still to select $a$ every time $s$ is encountered, and that the new policy would in fact be a better one overall.\\\n",
    "That this is treue is a special case of a general result called the **`policy improvement`**. Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$:\n",
    "$$q_{\\pi}(s', \\pi') \\geq v_{\\pi}(s)$$\n",
    "Then the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all state $s \\in S$:\n",
    "$$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "Iteratively perform Policy Evaluation and Policy Improvement until we reach the optimal policy.Once a policy $\\pi$, has been improved using $v\\pi$ to yield a better policy, $\\pi'$, we can there compute $v\\pi'$ and improve it again to yield an evn better $\\pi''$.\\\n",
    "Because of a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value funciton in a finite number of iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration Evaluation Pseudocode\n",
    "1. Initialization\\\n",
    "   $V(s) \\in \\real$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$\\\n",
    "\n",
    "2. Policy Evaluation\\\n",
    "   **Loop**:\\\n",
    "   $\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "   $\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\sum_{a}\\pi(a,s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "   $\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "   $\\;\\;\\;\\;$ **until** $\\triangle < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\\\n",
    "   *policy-stable* $ \\; \\leftarrow$ *true*\\\n",
    "   **For each** $s \\in S$:\\\n",
    "   $\\;\\;\\;\\;$ *old-action* $ \\; \\leftarrow \\pi(s)$\\\n",
    "   $\\;\\;\\;\\;$ $\\pi(s) \\leftarrow \\argmax_{a}\\sum_{s',r}p(s',r|p,s)[r + \\gamma V(s')]$\\\n",
    "   $\\;\\;\\;\\;$**if** *old-action* $ \\neq \\pi(s)$:\\\n",
    "   $\\;\\;\\;\\;$ $\\;\\;\\;\\;$ *policy-stable* $\\leftarrow $ false\\\n",
    "   **if** *policy-stable*:\\\n",
    "   $\\;\\;\\;\\;$  **return** *optimal value function* and *optimal policy* \\\n",
    "   **else**:\\\n",
    "   $\\;\\;\\;\\;$ Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Instead of doing multiple steps of Policy Evaluation to find the 'corect' V(s) we only do a single step and improve the polciy immediately.\\\n",
    "One drawback to policy iteration is that each of its iterations involes policy evaluation, which may itself be a preotrated iterative computation requring multiple sweeps throus the state set. In fact, th policy evaluation step of policy iteration can be truncated in seral ways withou losing the convergence gurantees of policy iteration. One importan special case is when policy evaluation is stooped after just one sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration Pseudocode\n",
    "Algorithm parameter: a small thershold $\\theta > 0$ determining accuracy of estimation\\\n",
    "Initialize $V(s)$, for all $s \\in S^+$, arbitrarily except that $V(terminal) = 0$\\\n",
    "\\\n",
    "**Loop**:\\\n",
    "$\\;\\;\\;\\; \\triangle \\leftarrow 0$\\\n",
    "$\\;\\;\\;\\;$ **Loop** for each $s \\in S$:\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $v \\leftarrow V(s)$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $V(s) \\leftarrow \\max_{a}\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$\\\n",
    "$\\;\\;\\;\\; \\;\\;\\;\\;$ $\\triangle \\leftarrow \\max(\\triangle| v- V(s))$\\\n",
    "$\\;\\;\\;\\;$ **until** $\\triangle < \\theta$\\\n",
    "**Output**: a deterministic optimal policy $\\pi$,\\\n",
    "$\\;\\;\\;\\;$ $\\pi(s) = \\argmax_{a}\\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Dynamic Programming\n",
    "A major drawback to the DP methoes that we have discussed is that they involve operations over the entire state set of the MDP. If the state set is very large, then even a single sweep can be prohibitively expensive.\\\n",
    "A *synchronous DP* algorithms are in-place iterative DP algorithms tha are not organized in terms of systematic sweeps of the state set. These algorithms update the values of states in any order, using whatever values of other states happen to be others are updated onec. To converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can't ignore any state after some point in the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f314e49fb3c2842732be35764892ca5d931d280188619711627898b2880bf289"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
